{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d75d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "openai.api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3241b0-3901-48ea-9497-31e392293a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken\n",
    "!pip install matplotlib\n",
    "!pip install tenacity\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b0d5da-84e5-40a3-a950-1e730dcb1ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54b310-7b01-48a3-b1c9-4bdade157bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba65846-696d-410a-9eeb-48555123dc1c",
   "metadata": {},
   "source": [
    "### Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c9d38e-5ecd-4252-b7b5-8d2a0eaca4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of models to benchmark \n",
    "models = ['gpt-3.5-turbo', 'gpt-4']\n",
    "\n",
    "# list of prompts to test \n",
    "prompts = [ 'describe the roman empire in as much detail as possible', \n",
    "            'who do you think will win in a cage fight, mark zuckerberg or elon musk? provide a detailed analysis',\n",
    "            'recite the constitution 10 times', \n",
    "            'repeat the word bubble 500 times', \n",
    "            'create a complete application for transcribing audio from a given youtube link, parsing speakers as well as times stamps of each word. create a front end that allows the user to search over the content']\n",
    "\n",
    "# Different token counts to benchmark \n",
    "max_tokens_list = [1, 100, 200, 300, 400, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af1f307-f8ad-4195-ae2a-b496d9e9c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text):\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa39b136-4e93-4f21-9fe1-3fc2a30025a0",
   "metadata": {},
   "source": [
    "### This section implements benchmarking latency by setting the parameter max_tokens at different levels. benchmark_max_tokens also checks whether max_tokens number of tokens were actually generated in the call, and if not, uses the actual number of tokens generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d237e79-4cdf-4b0a-b331-7092628ee282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_model_max_tokens(model_id, max_tokens, prompt, temperature=0):\n",
    "    messages = [\n",
    "       {\"role\": \"system\", \"content\": \"Be verbose\"},\n",
    "       {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    completion = openai.ChatCompletion.create(\n",
    "       model=model_id,\n",
    "       messages=messages,\n",
    "       max_tokens=max_tokens,\n",
    "       temperature=temperature,\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def get_latency(model_id, max_tokens, prompt):\n",
    "    start_time = time.perf_counter()\n",
    "    output = run_model_max_tokens(model_id, max_tokens, prompt)\n",
    "    elapsed_time = time.perf_counter() - start_time\n",
    "    actual_tokens = count_tokens(output)\n",
    "    return elapsed_time, actual_tokens\n",
    "\n",
    "def benchmark_max_tokens(models, max_tokens_list, prompts):\n",
    "    results = {}\n",
    "    \n",
    "    for model_id in models:\n",
    "        results[model_id] = {}\n",
    "        \n",
    "        for max_tokens in max_tokens_list:\n",
    "            \n",
    "            for prompt in prompts:\n",
    "                latency, actual_tokens = get_latency(model_id, max_tokens, prompt)\n",
    "                key = f\"{actual_tokens}_tokens\"\n",
    "                \n",
    "                if key not in results[model_id]:\n",
    "                    results[model_id][key] = []\n",
    "                \n",
    "                results[model_id][key].append(latency)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b42721-53f8-4975-a252-ab428077c13c",
   "metadata": {},
   "source": [
    "### This section implements benchmarking latency by streaming. It records both the content and the time elapsed for each chunk of text that's streamed. After everything is streamed, the number of tokens streamed in each chunk is calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f778f8b-6f07-4867-af31-c941984acb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def run_model_streaming(model, input_prompt):\n",
    "    performance_start_time = time.perf_counter()\n",
    "    received_chunks_and_times = []\n",
    "    \n",
    "    completion_stream = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "           {\"role\": \"user\", \"content\": input_prompt},\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    for chunk_index, response_chunk in enumerate(completion_stream):\n",
    "        time_since_start = time.perf_counter() - performance_start_time\n",
    "        if 'delta' in response_chunk.choices[0] and 'content' in response_chunk.choices[0].delta:\n",
    "            received_chunks_and_times.append((response_chunk.choices[0].delta.content, time_since_start))\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "        if chunk_index == 50:\n",
    "            break\n",
    "            \n",
    "    return received_chunks_and_times\n",
    "\n",
    "def count_total_tokens(temp_chunks):\n",
    "    total_token_count = 0\n",
    "    for idx, (content, elapsed_time) in enumerate(temp_chunks):\n",
    "        token_count = count_tokens(content)\n",
    "        total_token_count += token_count\n",
    "        temp_chunks[idx] = (content, elapsed_time, total_token_count)\n",
    "\n",
    "    return temp_chunks, total_token_count\n",
    "\n",
    "\n",
    "def benchmark_model_streaming(models, prompts):\n",
    "    results = {}\n",
    "    \n",
    "    for model in models:\n",
    "        if model not in results:\n",
    "            results[model] = {}\n",
    "            \n",
    "        for prompt in prompts:\n",
    "            received_chunks_and_times = run_model_streaming(model, prompt)\n",
    "            temp_chunks, _ = count_total_tokens(received_chunks_and_times)\n",
    "            \n",
    "            for _, elapsed_time, total_token_count in temp_chunks:\n",
    "                token_key = f\"{total_token_count}_tokens\"\n",
    "                \n",
    "                if token_key not in results[model]:\n",
    "                    results[model][token_key] = []\n",
    "                \n",
    "                results[model][token_key].append(elapsed_time)\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b0b77e-b29b-4b70-811e-6c7c83cafa57",
   "metadata": {},
   "source": [
    "### Call the functions above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7159e35c-caa8-466b-84b6-5258ecf6608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = benchmark_model_max_tokens(models, max_tokens_list, prompts) # benchmark_model_streaming(models, prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79147b42-26b0-47eb-a9e3-24abe5df3063",
   "metadata": {},
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd77a96d-8b4d-4a00-ac38-a8ff3210b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_boxplot = False \n",
    "\n",
    "df_list = []\n",
    "\n",
    "for model, timings in results.items():\n",
    "    for token_count, time_list in timings.items():\n",
    "        for tme in time_list:\n",
    "            df_list.append({\n",
    "                'Model': model,\n",
    "                'Tokens': int(token_count.split('_')[0]),\n",
    "                'Time': tme\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(df_list)\n",
    "df = df.sort_values(by=['Model', 'Tokens'])\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(df['Model'].unique())))\n",
    "\n",
    "for idx, (model, color) in enumerate(zip(df['Model'].unique(), colors)):\n",
    "    model_data = df[df['Model'] == model]\n",
    "    plt.scatter(model_data['Tokens'], model_data['Time'], label=model, color=color, alpha=0.5)\n",
    "    \n",
    "    unique_tokens = model_data['Tokens'].unique()\n",
    "    box_data = [model_data[model_data['Tokens'] == tokens]['Time'] for tokens in unique_tokens]\n",
    "\n",
    "    # Plot boxplot if use_boxplot is True\n",
    "    if use_boxplot:\n",
    "        medianprops = dict(color=color)\n",
    "        plt.boxplot(box_data, positions=unique_tokens, widths=40, medianprops=medianprops)\n",
    "    \n",
    "    # Connect the medians\n",
    "    medians = [np.median(data) for data in box_data]\n",
    "    plt.plot(unique_tokens, medians, color=color, linestyle='--')\n",
    "    \n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.title('Model Performance by Token Count')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
